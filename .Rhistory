aes(x=year,
y=year_change,
group=channel,
color=channel))+
geom_line()+
theme_bw()
p_lag
## Clean up:
rm(tmp_lag,p_lag)
## Make the lag:
## (Separate for readability)
spend_year_dif <- spend_by_year %>%
group_by(channel) %>%
arrange(year, .by_group = TRUE) %>%
mutate(year_change = total_channel_spend - lag(total_channel_spend,
default = first(total_channel_spend)))
## Order and extract:
tmp_lag <- spend_year_dif[which(spend_year_dif$year==2022),]
print("The channel with the largest spending increase from 2021 to 2022 is:")
print(tmp_lag[order(tmp_lag$year_change, decreasing=TRUE),][1,c("channel", "year_change")])
## Visualize the channel-by-channel spending patterns:
p_lag <-  ggplot(spend_year_dif,
aes(x=year,
y=year_change,
group=channel,
color=channel))+
geom_line()+
theme_bw()
p_lag
## Clean up:
rm(tmp_lag,p_lag)
## Make the lag:
## (Separate for readability)
spend_year_dif <- spend_by_year %>%
group_by(channel) %>%
arrange(year, .by_group = TRUE) %>%
mutate(year_change = total_channel_spend - lag(total_channel_spend,
default = first(total_channel_spend)))
## Order and extract:
tmp_lag <- spend_year_dif[which(spend_year_dif$year==2022),]
print("The channel with the largest spending increase from 2021 to 2022 is:")
print(tmp_lag[order(tmp_lag$year_change, decreasing=TRUE),][1,c("channel", "year_change")])
## Visualize the channel-by-channel spending patterns:
p_lag <-  ggplot(spend_year_dif,
aes(x=year,
y=year_change,
group=channel,
color=channel))+
geom_line()+
theme_bw()
p_lag
## Clean up:
rm(tmp_lag,p_lag)
print(tmp_lag[order(tmp_lag$year_change, decreasing=TRUE),][1,c("channel", "year_change")])
## Make the lag:
## (Separate for readability)
spend_year_dif <- spend_by_year %>%
group_by(channel) %>%
arrange(year, .by_group = TRUE) %>%
mutate(year_change = total_channel_spend - lag(total_channel_spend,
default = first(total_channel_spend)))
## Order and extract:
tmp_lag <- spend_year_dif[which(spend_year_dif$year==2022),]
print("The channel with the largest spending increase from 2021 to 2022 is:")
print(tmp_lag[order(tmp_lag$year_change, decreasing=TRUE),][1,c("channel", "year_change")])
## Visualize the channel-by-channel spending patterns:
p_lag <-  ggplot(spend_year_dif,
aes(x=year,
y=year_change,
group=channel,
color=channel))+
geom_line()+
theme_bw()
p_lag
## Clean up:
rm(tmp_lag,p_lag)
## Make the lag:
## (Separate for readability)
spend_year_dif <- spend_by_year %>%
group_by(channel) %>%
arrange(year, .by_group = TRUE) %>%
mutate(year_change = total_channel_spend - lag(total_channel_spend,
default = first(total_channel_spend)))
## Order and extract:
tmp_lag <- spend_year_dif[which(spend_year_dif$year==2022),]
print("The channel with the largest spending increase from 2021 to 2022 is:")
print(tmp_lag[order(tmp_lag$year_change, decreasing=TRUE),][1,c("channel", "year_change")])
## Visualize the channel-by-channel spending patterns:
p_lag <-  ggplot(spend_year_dif,
aes(x=year,
y=year_change,
group=channel,
color=channel))+
geom_line()+
theme_bw()
p_lag
## Clean up:
rm(tmp_lag,p_lag)
print("The channel with the largest spending increase from 2021 to 2022 is:")
tmp_lag <- spend_year_dif[which(spend_year_dif$year==2022),]
tmp_lag <- tmp_lag[order(tmp_lag$year_change, decreasing=TRUE),]
print(tmp_lag)
table(spend$channel, spend$year)
summary(spend$date)
spend_slice = spend[spend$date >= "2021-01-01" &
spend$date <= "2021-10-31",]
spend_slice2021 = spend[spend$date >= "2021-01-01" &
spend$date <= "2021-10-31",]
spend_slice2022 = spend[spend$date >= "2022-01-01" &
spend$date <= "2022-10-31",]
table(spend_slice2021$channel, spend_slice2021$year)
cbind(table(spend_slice2021$channel,
spend_slice2021$year),
table(spend_slice2022$channel,
spend_slice2022$year))
spend_slice <- rbind(spend_slice_2021, spend_slice2022)
spend_slice <- rbind(spend_slice2021, spend_slice2022)
rm(spend_slice2021, spend_slice2022)
View(spend_slice)
spend_by_year <- spend_slice %>%
group_by(year, channel) %>%
mutate(total_channel_spend=sum(spend),
count=n()) %>%
distinct(year, channel, total_channel_spend, count)
View(spend_year_dif)
View(spend_by_year)
spend_by_year <- spend_slice %>%
group_by(year, channel) %>%
mutate(total_channel_spend=sum(spend),
count=n()) %>%
distinct(year, channel, total_channel_spend)
check1 = spend_by_year[which(spend_by_year$year==2021 &
spend_by_year$channel == 'tiktok'),
"total_channel_spend"]
check1 = spend_by_year[which(spend_by_year$year==2021 &
spend_by_year$channel == 'tiktok'),
"total_channel_spend"]
check2 = sum(spend[which(spend$year==2021 & spend$channel == 'tiktok'),c("spend")])
check1==check2 ## TRUE
check2 = sum(spend_slice[which(spend_slice$year==2021 & spend$_slicechannel == 'tiktok'),c("spend")])
check1==check2 ## TRUE
check2 = sum(spend_slice[which(spend_slice$year==2021 &
spend_slice$channel == 'tiktok'),c("spend")])
check1==check2 ## TRUE
rm(check2, check1)
spend_year_dif <- spend_by_year %>%
group_by(channel) %>%
arrange(year, .by_group = TRUE) %>%
mutate(year_change = total_channel_spend - lag(total_channel_spend,
default = first(total_channel_spend)))
print("The channel with the largest spending increase from 2021 to 2022 is:")
tmp_lag <- spend_year_dif[which(spend_year_dif$year==2022),]
print(tmp_lag[order(tmp_lag$year_change, decreasing=TRUE),][1:2,c("channel", "year_change")])
p_lag <-  ggplot(spend_year_dif,
aes(x=year,
y=year_change,
group=channel,
color=channel))+
geom_line()+
theme_bw()
p_lag
print(tmp_lag[order(tmp_lag$year_change, decreasing=TRUE),][1:2,c("channel", "year_change")])
p_lag <-  ggplot(spend_year_dif,
aes(x=year,
y=year_change,
group=channel,
color=channel))+
geom_line()+
ylab("Spend Change") +
xlab("Year")
theme_bw()
p_lag <-  ggplot(spend_year_dif,
aes(x=year,
y=year_change,
group=channel,
color=channel))+
geom_line()+
ylab("Spend Change") +
xlab("Year")+
theme_bw()
p_lag
View(revenue)
revenue$daytot = sum(revenue[, c("revenue_dtc", "revenue_amazon", "revenue_walmart")])
revenue$daytot = sum(revenue[, 2:4])
revenue$daytot = rowSums(revenue[, 2:4])
cols_rev= c("revenue_dtc", "revenue_amazon", "revenue_walmart")
revenue$daytot = rowSums(revenue[, cols_rev])
revenue$zscore <- scale(revenue$daytot)
zscore <- scale(revenue$daytot)
View(zscore)
revenue$zscore <- as.vector(scale(revenue$daytot))
revenue[which(revenue$zscore>=3 | revenue$zscore<=3),]
revenue[which(revenue$zscore>= 3 | revenue$zscore<= -3),]
summary(revenue$zscore)
revenue[which(revenue$zscore >= 1.5 | revenue$zscore<= -1.5),]
revenue[which(revenue$zscore = max(revenue$zscore),]
revenue[which(revenue$zscore == max(revenue$zscore),]
revenue[which(revenue$zscore == max(revenue$zscore),]
revenue[which(revenue$zscore == max(revenue$zscore)),]
revenue[which(revenue$zscore == max(revenue$zscore)),]
print(revenue[which(revenue$zscore == max(revenue$zscore)),])
summary(revenue$zscore)
print(revenue[which(revenue$zscore == 1.95544),])
revenue[which(revenue$zscore == 1.95544),]
revenue[which(revenue$zscore > 1.9),]
install.packages(forecast)
install.packages("forecast"")
library(forecast)
install.packages("forecast")
tsoutliers(revenue)
library(forecast)
tsoutliers(revenue)
tsoutliers(revenue$daytot)
revenue[475,]
print(revenue[475,])
print(revenue$date[475])
library(forecast)
tsoutliers(revenue$daytot) ## returns index 475.
print(revenue$date[475])
tsoutliers(revenue$daytot)$index ## returns index 475.
print(paste0("The large revenue spike occured on: ", revenue$date[475]))
library(forecast)
outlier_ind = tsoutliers(revenue$daytot)$index ## returns position 475.
print(paste0("The large revenue spike occured on: ", revenue$date[outlier_ind]))
install.packages("tinytex")
install.packages("tinytex")
## Replication script for:
## Foster and Johnson,
## "Rhetorical Framing in Inter-Governmental Negotiations"
rm(list=ls())
loadPkg=function(toLoad){
for(lib in toLoad){
if(! lib %in% installed.packages()[,1])
{install.packages(lib,
repos='http://cran.rstudio.com/')}
suppressMessages( library(lib, character.only=TRUE))}}
packs <- c('pdftools',
"tidyr",
"tidytext",
"ggplot2",
"tidyverse",
"xtable",
'Hmisc',
"ggridges")
loadPkg(packs)
#########################
## Declare Data Paths
#########################
dataPath <- "./data/"
imagePath <- "./images/"
#############################
## Load Processed Text Data
#############################
##
data <- readxl::read_excel(paste0(dataPath,
"WTOSpeakerTurnsM1to113.xlsx"))
getwd()
getwd()
knitr::opts_chunk$set(
collapse = TRUE,
comment = "#>"
)
## IRT-M estimation:
library(devtools)
devtools::install_github("dasiegel/IRT-M")
install.packages("RCPP", dependencies=TRUE)
install.packages("Rcpp", dependencies=TRUE)
devtools::install_github("dasiegel/IRT-M")
install.packages("RcppArmadillo", dependencies =TRUE)
library(RcppArmadillo)
devtools::install_github("dasiegel/IRT-M")
install.packages("shiny")
install.packages(c('shiny', 'ggvis', 'dplyr', 'RSQLite'))
install.packages(c('golem'))
source("~/Dropbox (Personal)/Firstlight/ContinuingEd/Shiny-Tutorial/DC_Shiny.Rmd")
source("~/Dropbox (Personal)/Firstlight/ContinuingEd/Shiny-Tutorial/DC_Shiny.Rmd")
source("~/Dropbox (Personal)/Firstlight/ContinuingEd/Shiny-Tutorial/DC_Shiny.Rmd")
shiny::runApp('Dropbox (Personal)/WTO-Github/Gapminder')
runApp('Dropbox (Personal)/WTO-Github/greeting/multilingualHelloapp.R')
runApp('Dropbox (Personal)/WTO-Github/greeting/multilingualHelloapp.R')
langtab <- DataFrame(lang = c("French", "English"),
say = c("Bonjour", "Hello"))
langtab <- as.data.frame(lang = c("French", "English"),
say = c("Bonjour", "Hello"))
langtab <- dataframe(lang = c("French", "English"),
say = c("Bonjour", "Hello"))
langtab <- data.frame(lang = c("French", "English"),
say = c("Bonjour", "Hello"))
print(lagtab)
print(langtab)
runApp('Dropbox (Personal)/WTO-Github/greeting/multilingualHelloapp.R')
runApp('Dropbox (Personal)/WTO-Github/greeting/multilingualHelloapp.R')
runApp('Dropbox (Personal)/WTO-Github/greeting/multilingualHelloapp.R')
runApp('Dropbox (Personal)/WTO-Github/greeting/multilingualHelloapp.R')
runApp('Dropbox (Personal)/WTO-Github/greeting/multilingualHelloapp.R')
runApp('Dropbox (Personal)/WTO-Github/greeting/multilingualHelloapp.R')
runApp('Dropbox (Personal)/WTO-Github/greeting/multilingualHelloapp.R')
install.packages
install.packages('babynames')
library(babynames)
summary(babynames)
min(babynames$year)
print(unique(babynames$sex))
runApp('Dropbox (Personal)/WTO-Github/PopularNames/popularNamesapp.R')
runApp('Dropbox (Personal)/WTO-Github/PopularNames/popularNamesapp.R')
runApp('Dropbox (Personal)/WTO-Github/PopularNames/popularNamesapp.R')
runApp('Dropbox (Personal)/WTO-Github/PopularNames/popularNamesapp.R')
runApp('Dropbox (Personal)/WTO-Github/PopularNames/popularNamesapp.R')
tmp <- babynames %>% filter(year == 2005) %>% filter(sex == "F")
print(tmp)
ggplot(tmp, aes(name))
runApp('Dropbox (Personal)/WTO-Github/PopularNames/popularNamesapp.R')
runApp('Dropbox (Personal)/WTO-Github/PopularNames/popularNamesapp.R')
runApp('Dropbox (Personal)/WTO-Github/PopularNames/popularNamesapp.R')
getwd()
getwd*()
getwd()
install.packages('blavaan')
knitr::opts_chunk$set(
collapse = TRUE,
comment = "#>"
)
## Data prep:
library(tidyverse) # version: tidyverse_2.0.0
library(dplyr) #version: dplyr_1.1.4
library(stats) # version: stats4
library(fastDummies) # version: fastDummies_1.7.3
library(reshape2) #version: reshape2_1.4.4
## IRT-M estimation:
#devtools::install_github("dasiegel/IRT-M")
library(IRTM) #version 1.00
library(lavaan)
library(blavaan)
## Results visualization:
library(ggplot2)  # version: ggplot2_3.4.4
library(ggridges) #version: ggridges_0.5.6
library(RColorBrewer) #version: RColorBrewer_1.1-3
library(ggrepel) # version: ggrepel_0.9.5
## Data prep:
library(tidyverse) # version: tidyverse_2.0.0
library(dplyr) #version: dplyr_1.1.4
library(stats) # version: stats4
library(fastDummies) # version: fastDummies_1.7.3
library(reshape2) #version: reshape2_1.4.4
## IRT-M estimation:
#devtools::install_github("dasiegel/IRT-M")
library(IRTM) #version 1.00
library(lavaan) #CFA and SEM
library(blavaan) # BSEM
## Results visualization:
library(ggplot2)  # version: ggplot2_3.4.4
library(ggridges) #version: ggridges_0.5.6
library(RColorBrewer) #version: RColorBrewer_1.1-3
library(ggrepel) # version: ggrepel_0.9.5
To begin, we reformat data so that each possible answer becomes a separate binary question (One Hot encoding). In preparing the data, we used the `dummy_cols()` utility from the `fastdummies` package.  Finally, we rename the new binary dataframe as `Y` to underscore that this is the observed data that we will be modeling. Please ensure that the `dataPath` variable is adjusted for your local file structure.
## MODIFICATION FOR SYNTHETIC DATA
datapath <- "../data/"
ebdatsynth <- read.csv(file=paste0(datapath, 'synth_questions.csv'))
colnames(ebdatsynth)
## drop "X" and "unnamed" which are just row counters
ebdatsynth[,c("X", "Unnamed..0")] <- NULL
## Convert numeric ordinal responses to factors
ebdatsub <- lapply(ebdatsynth[,], factor) ## that's a list now
## converts the list back into a dataframe:
ebbinary <- fastDummies::dummy_cols(.data=ebdatsub,
remove_selected_columns=TRUE)
Y <- ebbinary ## data
## remove the .data that dummy_cols adds to the column names
colnames(Y) <- gsub(".data.", '', colnames(Y))
## remove the data objects:
rm(ebdatsub)
rm(ebdatsynth)
load(file=paste0(datapath, 'mcodes.rda'))
## Only keep M-Codes with loadings or outcomes:
MCodes$encoding <- rowSums(abs(MCodes[,4:9]))
MCodes <- MCodes[which(MCodes$encoding > 0),]
print(colnames(MCodes))
print(head(MCodes))
MCodes[which(MCodes$`D1-Culture threat`==1),]$QMap
MCodes[which(MCodes$`D2-ReligionThreat`==1),]$QMap
## Load on Economic Threat
MCodes[which(MCodes$`D3-Economic Threat`==1),]$QMap
MCodes[which(MCodes$`D4-HealthThreat`==1),]$QMap
paste(MCodes[which(MCodes$`D1-Culture threat`==1),]$QMap)
# Load on Culture Threat:
paste(MCodes[which(MCodes$`D1-Culture threat`==1),]$QMap, sep = " + ")
# Load on Culture Threat:
paste(MCodes[which(MCodes$`D1-Culture threat`==1),]$QMap, sep = " + ", collapse = TRUE)
# Load on Culture Threat:
paste(MCodes[which(MCodes$`D1-Culture threat`==1),]$QMap, sep = " + ", collapse = "+")
MCodes[which(MCodes$`D3-Economic Threat`==1),]$QMap
paste(MCodes[which(MCodes$`D1-Culture threat`==1),]$QMap, collapse = " + ")
rm(list = ls())
setwd("~/Dropbox/DominosPaper/Dominos-Code/code2022/")
library(dplyr)
library(ggplot2)
library(igraph)
## Format data:
## (Reconstruct the initial network statistics for the networks)
## Create initial centrality measure
## and number of rounds to entry into group
process <- function(filename, netstype){
load(filename)
netstype= netstype ## code network types
exit= data.frame()
entry= data.frame()
alldat = data.frame()
## Reconstruct initial network statistics
for(r in 1:length(node.traj)){
print(r)
tmp = bind_rows(node.traj[[r]])
## fallthrough if the node.traj object doesn't
## have 11 columns [indicates a corner case]
if(!dim(tmp)[2]==11){
print(paste0("skipping round ", r, "for dim incompatibility"))
next}
## Recreate initial centrality values
## for group members (g nodes) & recruits (r nodes)
g1 <-  graph_from_edgelist(
as.matrix(bN[[r]]$edgelist[grepl(pattern="g",
x=bN[[r]]$edgelist$from),
c("from", "to")]))
gcent <- round(eigen_centrality(g1)$vector, 2)
r1 <-  graph_from_edgelist(
as.matrix(bN[[r]]$edgelist[grepl(pattern="r",
x=bN[[r]]$edgelist$from),
c("from", "to")]))
rcent <- round(eigen_centrality(r1)$vector, 2)
## Graph-level centralization:
rcent.eigen <- centr_eigen(r1)$centralization
rcent.degree <- centr_degree(r1)$centralization
gcent.eigen <- centr_eigen(g1)$centralization
gcent.degree <- centr_degree(g1)$centralization
## move to a dataframe:
init.centrality <- as.data.frame(c(gcent, rcent))
init.centrality$nodeID <- as.character(rownames(init.centrality))
init.centrality$ggraph.ecent <- round(gcent.eigen, 3)
init.centrality$ggraph.degcent <- round(gcent.degree, 3)
init.centrality$rgraph.ecent <-   round(rcent.eigen, 3)
init.centrality$rgraph.degcent <- round(rcent.degree, 3)
colnames(init.centrality)[1:2] <- c("initialcent", "nodeID")
init.centrality$whichSim <- r
## get an initial metric centralization score:
## for recruits and for group members:
## Simulation data
## Observe that I'm throwing away a lot of data in the second index;
##need to figure out what
to.work.with <- node.traj[[r]] ## list of node attributes by round
tww <- bind_rows(to.work.with,## list to dataframe
.id = "column_label")
tww2 <- left_join(tww, ## add in the centrality info
init.centrality,
by=c("nodeID"= "nodeID"))
tww2$structure = netstype ## TWW2 is one round
alldat = rbind(alldat, tww2) ## alldat is nodes for all rounds
} ## end node info reconstruction
## dignostics, if desired
##plot(density(tww2$ggraph.ecent)) ## uniform
##plot(density(tww2$rgraph.ecent)) ## uniform
##table(tww2$iteration)
##table(tww2$round)
##table(tww2$nodeID) ##  63 per node, except g17(?)
##table(tww2$shockpercent) ## 10-90
## clean up into a dataframe:
dev.alldat <-alldat %>%
group_by(iteration, shockpercent, round) %>%
count(type) %>%
mutate(total.nodes = sum(n)) %>%
mutate(rep.percent = round(n/total.nodes, 2))#
## add the graph centralization info:
## list the graph attribute columns:
## get the columns with "graph" in the name:
## (these are graph attributes, and I don't feel like typing all)
gas <-colnames(alldat)[grep(colnames(alldat), pattern="graph")]
graph.atts <- c("iteration", "shockpercent", "round",
gas)
## add original network centrality attributes:
dev.alldat2 <- dev.alldat %>%
left_join(unique(alldat[,graph.atts]),
by=c("iteration" = "iteration", ##reminder: iteration = which simulated network
"shockpercent" = "shockpercent",
"round" = "round"))
## Add the network configuration:
dev.alldat2$nettype <- netstype
return(dev.alldat2)
}
erpa_nets <- process(filename="ER-PASims.Rdata",
netstype="ER-PA")
View(erpa_nets)
gg2 <- ggplot(data = er_pa,
aes(y = rep.percent,
x = round))+
geom_boxplot(aes(group= round), outlier.alpha=.1)+
facet_grid(type ~shockpercent)+
theme_bw()+
scale_x_discrete(limits=c(paste0(1:7)))+
theme(legend.position="bottom")
gg2 <- ggplot(data =erpa_nets,
aes(y = rep.percent,
x = round))+
geom_boxplot(aes(group= round), outlier.alpha=.1)+
facet_grid(type ~shockpercent)+
theme_bw()+
scale_x_discrete(limits=c(paste0(1:7)))+
theme(legend.position="bottom")
gg2
devtools::check(IRT)
devtools::check(IRTM)
setwd("~/Dropbox/IRTM-Pkg/IRTM")
install.packages(c("devtools", "rcmdcheck", "rhub", "revdepcheck"))
devtools::check()
devtools::check()
